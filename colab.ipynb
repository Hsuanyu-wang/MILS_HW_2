{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/colab.ipynb\n",
    "import os\n",
    "# Change to the MILS_HW2 directory first\n",
    "os.chdir('../MILS_HW2')\n",
    "\n",
    "# Cell 1: Setup and downloads\n",
    "%pip install -r requirements.txt\n",
    "# Download all datasets\n",
    "%python scripts/download_imagenette_cls.py\n",
    "%python scripts/download_coco_det.py\n",
    "%python scripts/download_voc_seg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1686eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying downloads...\n",
      "seg dataset found at ./data/VOCdevkit/VOC2012\n",
      "det dataset found at ./data/coco_subset\n",
      "cls dataset found at ./data/imagenette2-160\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Verify downloads\n",
    "# Add a verification step\n",
    "print(\"Verifying downloads...\")\n",
    "import os\n",
    "data_paths = {\n",
    "    'seg': './data/VOCdevkit/VOC2012',\n",
    "    'det': './data/coco_subset',\n",
    "    'cls': './data/imagenette2-160'\n",
    "}\n",
    "\n",
    "for task, path in data_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"{task} dataset found at {path}\")\n",
    "    else:\n",
    "        print(f\"WARNING: {task} dataset not found at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc7253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions initialized\n",
      "Total parameters: 3.2M\n",
      "Loading datasets...\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/coco_subset/annotations/instances_train2017.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 創建數據載入器 (根據你的下載檔案)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloaders\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataloaders)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 準備datasets字典給trainer使用 (只用train set)\u001b[39;00m\n",
      "File \u001b[0;32m/home/MILS_HW2/src/datasets/data_loaders.py:150\u001b[0m, in \u001b[0;36mcreate_dataloaders\u001b[0;34m(batch_size, num_workers)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataloaders\u001b[39m(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"創建DataLoaders\"\"\"\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_datasets_from_downloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdet\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/home/MILS_HW2/src/datasets/data_loaders.py:130\u001b[0m, in \u001b[0;36mcreate_datasets_from_downloads\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m imagenette_val_dataset \u001b[38;5;241m=\u001b[39m ClassificationDataset(imagenette_val_subset)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# 3. COCO Detection (根據你的download_coco_det.py)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m coco_train \u001b[38;5;241m=\u001b[39m \u001b[43mCOCODetectionDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/coco_subset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m coco_val \u001b[38;5;241m=\u001b[39m COCODetectionDataset(\n\u001b[1;32m    137\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/coco_subset\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    138\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    139\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseg\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: voc_train_subset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: voc_val_subset},\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: imagenette_train_dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: imagenette_val_dataset}, \n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdet\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: coco_train, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: coco_val}\n\u001b[1;32m    146\u001b[0m }\n",
      "File \u001b[0;32m/home/MILS_HW2/src/datasets/data_loaders.py:40\u001b[0m, in \u001b[0;36mCOCODetectionDataset.__init__\u001b[0;34m(self, root_dir, split, transform)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 載入COCO annotations (根據你的下載檔案)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m ann_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstances_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m2017.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco \u001b[38;5;241m=\u001b[39m \u001b[43mCOCO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 你下載的10個類別\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcar\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicycle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmotorcycle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mairplane\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruck\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraffic light\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycocotools/coco.py:81\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading annotations into memory...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     82\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation file format \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(dataset))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/coco_subset/annotations/instances_train2017.json'"
     ]
    }
   ],
   "source": [
    "# Cell 3: Model and Data initialization \n",
    "from src.models.unified_model import UnifiedModel\n",
    "from src.datasets.data_loaders import create_dataloaders\n",
    "from src.training.loss_functions import MultiTaskLoss, UncertaintyWeightedLoss\n",
    "from configs.config import Config  # 使用Config類\n",
    "\n",
    "# 初始化配置\n",
    "config = Config()  # 創建Config實例，不是模組\n",
    "\n",
    "# 初始化損失函數\n",
    "criterion = MultiTaskLoss()\n",
    "print(\"Loss functions initialized\")\n",
    "\n",
    "# 初始化模型\n",
    "model = UnifiedModel()\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
    "\n",
    "# 創建數據載入器 (根據你的下載檔案)\n",
    "print(\"Loading datasets...\")\n",
    "dataloaders = create_dataloaders(\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers\n",
    ")\n",
    "print(\"dataloaders\", dataloaders)\n",
    "\n",
    "# 準備datasets字典給trainer使用 (只用train set)\n",
    "datasets = {\n",
    "    'seg': dataloaders['seg']['train'],\n",
    "    'seg_val': dataloaders['seg']['val'],\n",
    "    'det': dataloaders['det']['train'],\n",
    "    'det_val': dataloaders['det']['val'],\n",
    "    'cls': dataloaders['cls']['train'],\n",
    "    'cls_val': dataloaders['cls']['val']\n",
    "}\n",
    "print(\"datasets:\\n\", datasets)\n",
    "print(\"Datasets loaded successfully!\")\n",
    "print(f\"Detection batches: {len(datasets['det'])}\")\n",
    "print(f\"Segmentation batches: {len(datasets['seg'])}\")\n",
    "print(f\"Classification batches: {len(datasets['cls'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Three-stage training\n",
    "from src.training.trainer import MultiTaskTrainer\n",
    "trainer = MultiTaskTrainer(model, datasets, config, criterion=criterion)\n",
    "\n",
    "# Stage 1: Segmentation baseline\n",
    "trainer.train_stage_1_segmentation()\n",
    "\n",
    "# Debug: Check dataloader lengths and try to fetch a batch\n",
    "print('Detection train batches:', len(datasets['det']))\n",
    "print('Classification train batches:', len(datasets['cls']))\n",
    "\n",
    "# Try to fetch a batch from detection\n",
    "try:\n",
    "    det_batch = next(iter(datasets['det']))\n",
    "    print('First detection batch:', det_batch)\n",
    "except Exception as e:\n",
    "    print('Could not fetch detection batch:', e)\n",
    "    det_batch = None\n",
    "\n",
    "# Try to fetch a batch from classification\n",
    "try:\n",
    "    cls_batch = next(iter(datasets['cls']))\n",
    "    print('First classification batch:', cls_batch)\n",
    "except Exception as e:\n",
    "    print('Could not fetch classification batch:', e)\n",
    "    cls_batch = None\n",
    "    \n",
    "print('Type of datasets[\\'det\\']:', type(datasets['det']))\n",
    "print('Type of datasets[\\'cls\\']:', type(datasets['cls']))\n",
    "    \n",
    "# Stage 2: Detection with EWC\n",
    "if det_batch is not None and len(datasets['det']) > 0:\n",
    "    trainer.train_stage_2_detection()\n",
    "else:\n",
    "    print('Skipping Stage 2: Detection dataloader is empty or invalid.')\n",
    "\n",
    "# Stage 3: Classification with replay\n",
    "if cls_batch is not None and len(datasets['cls']) > 0:\n",
    "    trainer.train_stage_3_classification()\n",
    "else:\n",
    "    print('Skipping Stage 3: Classification dataloader is empty or invalid.')\n",
    "\n",
    "# Validate forgetting constraint\n",
    "success = trainer.validate_forgetting_constraint()\n",
    "print(f\"Forgetting constraint satisfied: {success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Final evaluation\n",
    "!python scripts/eval.py --weights checkpoints/final_model.pt --dataroot data --tasks all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
